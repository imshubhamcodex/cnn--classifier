{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency imports\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PlantVillage dataset zip file\n",
    "\n",
    "url = \"https://data.mendeley.com/public-files/datasets/tywbtsjrjv/files/d5652a28-c1d8-4b76-97f3-72fb80f94efc/file_downloaded\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"plantvillage.zip\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Download failed! HTTP {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction dataset zip file\n",
    "\n",
    "zip_path = \"plantvillage.zip\"\n",
    "extract_dir = \"./plantvillage\"\n",
    "\n",
    "# Extract the dataset\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(\"Folder already exists, skipping extraction.\")\n",
    "\n",
    "# Path to the actual dataset folder (update if needed)\n",
    "dataset_root = './plantvillage/Plant_leave_diseases_dataset_without_augmentation'\n",
    "\n",
    "# List plant disease categories\n",
    "print(\"\\nPlant Disease Categories:\")\n",
    "if os.path.exists(dataset_root):\n",
    "    for disease_folder in sorted(os.listdir(dataset_root)):\n",
    "        if os.path.isdir(os.path.join(dataset_root, disease_folder)):\n",
    "            print(f\"- {disease_folder}\")\n",
    "else:\n",
    "    print(\"Dataset path not found! Check extraction folder structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eca068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced dataset directory\n",
    "\n",
    "original_dir = './plantvillage/Plant_leave_diseases_dataset_without_augmentation'\n",
    "balanced_dir = './plantvillage_balanced'\n",
    "os.makedirs(balanced_dir, exist_ok=True)\n",
    "\n",
    "# Image augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Count images per class\n",
    "class_counts = {\n",
    "    cls: len(os.listdir(os.path.join(original_dir, cls)))\n",
    "    for cls in os.listdir(original_dir)\n",
    "    if os.path.isdir(os.path.join(original_dir, cls))\n",
    "}\n",
    "\n",
    "max_count = max(class_counts.values())\n",
    "print(f\"Maximum images per class = {max_count}\")\n",
    "\n",
    "# Balance all classes\n",
    "for cls in sorted(os.listdir(original_dir)):\n",
    "    cls_path = os.path.join(original_dir, cls)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "\n",
    "    imgs = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    cls_bal_dir = os.path.join(balanced_dir, cls)\n",
    "    os.makedirs(cls_bal_dir, exist_ok=True)\n",
    "\n",
    "    # Copy original images\n",
    "    for img in imgs:\n",
    "        shutil.copy(os.path.join(cls_path, img), os.path.join(cls_bal_dir, img))\n",
    "\n",
    "    # Generate augmented images\n",
    "    i = 0\n",
    "    while len(os.listdir(cls_bal_dir)) < max_count:\n",
    "        img_name = random.choice(imgs)\n",
    "        img_path = os.path.join(cls_path, img_name)\n",
    "        img = Image.open(img_path).convert('RGB').resize((300, 300))\n",
    "        x = np.expand_dims(np.array(img) / 255.0, axis=0)\n",
    "        aug_img = next(datagen.flow(x, batch_size=1))[0]\n",
    "        aug_img = Image.fromarray((aug_img * 255).astype(np.uint8))\n",
    "        aug_img.save(os.path.join(cls_bal_dir, f\"aug_{i}_{img_name}\"))\n",
    "        i += 1\n",
    "\n",
    "print(\"Balanced dataset created at:\", balanced_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split balanced dataset into train, val, test\n",
    "\n",
    "# Paths\n",
    "balanced_dir = './plantvillage_balanced'\n",
    "split_dir = './plantvillage_split'\n",
    "os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Function to create split directories\n",
    "def make_dirs(base_dir, cls):\n",
    "    train_cls = os.path.join(base_dir, 'train', cls)\n",
    "    val_cls = os.path.join(base_dir, 'val', cls)\n",
    "    test_cls = os.path.join(base_dir, 'test', cls)\n",
    "    os.makedirs(train_cls, exist_ok=True)\n",
    "    os.makedirs(val_cls, exist_ok=True)\n",
    "    os.makedirs(test_cls, exist_ok=True)\n",
    "    return train_cls, val_cls, test_cls\n",
    "\n",
    "# Loop through each class and split\n",
    "for cls in sorted(os.listdir(balanced_dir)):\n",
    "    cls_path = os.path.join(balanced_dir, cls)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "\n",
    "    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    random.shuffle(images)\n",
    "\n",
    "    n_total = len(images)\n",
    "    n_train = int(train_ratio * n_total)\n",
    "    n_val = int(val_ratio * n_total)\n",
    "    # Remaining goes to test\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    train_cls, val_cls, test_cls = make_dirs(split_dir, cls)\n",
    "\n",
    "    # Copy images to respective folders\n",
    "    for i, img in enumerate(images):\n",
    "        src = os.path.join(cls_path, img)\n",
    "        if i < n_train:\n",
    "            dst = os.path.join(train_cls, img)\n",
    "        elif i < n_train + n_val:\n",
    "            dst = os.path.join(val_cls, img)\n",
    "        else:\n",
    "            dst = os.path.join(test_cls, img)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Dataset split into train, val, test at:\", split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f056e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation Script\n",
    "\n",
    "# ======================\n",
    "# === Parameters =======\n",
    "# ======================\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "NUM_TRAINABLE_LAYERS = 4  # last N layers to fine-tune (0 = no fine-tuning)\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# ======================\n",
    "# === Dataset Paths ====\n",
    "# ======================\n",
    "TRAIN_DATA_DIR = \"./plantvillage_split/train\"\n",
    "VALIDATION_DATA_DIR = \"./plantvillage_split/val\"\n",
    "TEST_DATA_DIR = \"./plantvillage_split/test\"\n",
    "\n",
    "# ======================\n",
    "# === Dynamic Class Detection ===\n",
    "# ======================\n",
    "class_labels = sorted([d for d in os.listdir(TRAIN_DATA_DIR) \n",
    "                       if os.path.isdir(os.path.join(TRAIN_DATA_DIR, d))])\n",
    "NUM_CLASSES = len(class_labels)\n",
    "print(f\"Detected {NUM_CLASSES} classes: {class_labels}\")\n",
    "\n",
    "# ======================\n",
    "# === Data Generators ===\n",
    "# ======================\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# === Build Model ======\n",
    "# ======================\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False,\n",
    "                            input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "base_model.summary()\n",
    "\n",
    "# Freeze all layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Unfreeze last N layers for fine-tuning\n",
    "if NUM_TRAINABLE_LAYERS > 0:\n",
    "    for layer in base_model.layers[-NUM_TRAINABLE_LAYERS:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "# Build the full model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# ======================\n",
    "# === Compile Model ====\n",
    "# ======================\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# ======================\n",
    "# === Callbacks ========\n",
    "# ======================\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                         patience=3, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n",
    "                                     restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"efficientnet_plantvillage_best.keras\",\n",
    "                                       monitor='val_accuracy',\n",
    "                                       save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# ======================\n",
    "# === Train Model ======\n",
    "# ======================\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# === Plot Training History ===\n",
    "# ======================\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# ======================\n",
    "# === Evaluate Model ===\n",
    "# ======================\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# ======================\n",
    "# === Confusion Matrix ===\n",
    "# ======================\n",
    "y_true = test_generator.classes\n",
    "y_pred_probs = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ======================\n",
    "# === Classification Report ===\n",
    "# ======================\n",
    "report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# ======================\n",
    "# === Per-Class Accuracy ===\n",
    "# ======================\n",
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)\n",
    "\n",
    "for true, pred in zip(y_true, y_pred):\n",
    "    class_total[class_labels[true]] += 1\n",
    "    if true == pred:\n",
    "        class_correct[class_labels[true]] += 1\n",
    "\n",
    "per_class_accuracy = {cls: class_correct[cls] / class_total[cls] for cls in class_labels}\n",
    "\n",
    "# Plot per-class accuracy\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.bar(per_class_accuracy.keys(), per_class_accuracy.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Per-Class Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Optional: print per-class accuracy sorted\n",
    "print(\"Per-Class Accuracy:\\n\")\n",
    "for cls, acc in sorted(per_class_accuracy.items(), key=lambda x: x[1]):\n",
    "    print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "# ======================\n",
    "# === Save Model =======\n",
    "# ======================\n",
    "model.save(\"efficientnet_plantvillage_final.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model for testing \n",
    "\n",
    "def predict_image_with_percentages(img_path, model, class_names):\n",
    "    # Load and preprocess\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Predict\n",
    "    preds = model.predict(img_array)[0]\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n========= Prediction Result =========\")\n",
    "    for cls, prob in zip(class_names, preds):\n",
    "        print(f\"{cls:25s} : {prob*100:.2f}%\")\n",
    "\n",
    "    predicted_class = class_names[np.argmax(preds)]\n",
    "    confidence = np.max(preds) * 100\n",
    "\n",
    "    print(\"\\nMost Likely Class:\", predicted_class)\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "\n",
    "    return predicted_class, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"efficientnet_plantvillage_final.keras\")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "\n",
    "# Load class names\n",
    "TEST_DATA_DIR = \"./plantvillage/Plant_leave_diseases_dataset_without_augmentation\"\n",
    "class_names = sorted(os.listdir(TEST_DATA_DIR))\n",
    "\n",
    "# Predict\n",
    "predict_image_with_percentages(\"real_time_testing/images2.jpg\", model, class_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
